{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modified from Part 2 #\n",
    "import codecs\n",
    "EN_train = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\EN\\train\"\n",
    "EN_modified = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\EN\\modified_train\"\n",
    "EN_test = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\EN\\dev.in\"\n",
    "EN_output = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\EN\\dev.p2.out\"\n",
    "EN_gold = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\EN\\dev.out\"\n",
    "EN_viterbi = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\EN\\dev.p3.out\"\n",
    "\n",
    "CN_train = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\CN\\train\"\n",
    "CN_modified = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\CN\\modified_train\"\n",
    "CN_test = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\CN\\dev.out\"\n",
    "CN_output = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\CN\\dev.p2.out\"\n",
    "CN_gold = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\CN\\dev.out\"\n",
    "CN_viterbi = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\CN\\dev.p3.out\"\n",
    "\n",
    "\n",
    "SG_train = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\SG\\train\"\n",
    "SG_modified = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\SG\\modified_train\"\n",
    "SG_test = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\SG\\dev.in\"\n",
    "SG_output = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\SG\\dev.p2.out\"\n",
    "SG_gold = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\SG\\dev.out\"\n",
    "SG_viterbi = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\SG\\dev.p3.out\"\n",
    "\n",
    "\n",
    "FR_train = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\FR\\train\"\n",
    "FR_modified = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\FR\\modified_train\"\n",
    "FR_test = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\FR\\dev.in\"\n",
    "FR_output = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\FR\\dev.p2.out\"\n",
    "FR_gold = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\FR\\dev.out\"\n",
    "FR_viterbi = r\"C:\\Users\\Matthew\\Desktop\\SUTD\\Term 6\\Machine Learning\\ML Project\\FR\\dev.p3.out\"\n",
    "\n",
    "def modified_training_set(train_file,modified_train_set):\n",
    "    #fout=open ('modified_train_set','w')\n",
    "    with open (train_file, encoding='utf-8') as file:\n",
    "        tag_count={}\n",
    "        modified_words=[]\n",
    "        for line in file:\n",
    "            pair=line.split()\n",
    "            if len(line.split())!=0:\n",
    "                tag=pair[0]\n",
    "                observ=pair[1]\n",
    "                if tag in tag_count.keys():\n",
    "                    tag_count[tag]+=1\n",
    "                else:\n",
    "                    tag_count[tag]=1\n",
    "        for tag in tag_count:\n",
    "            if tag_count[tag]<3:\n",
    "                modified_words.append(tag)\n",
    "    with open (train_file, encoding='utf-8') as file2, codecs.open(modified_train_set, 'w', 'utf-8-sig') as fout:\n",
    "        for line in file2:\n",
    "            pair2=line.split()\n",
    "            if len(line.split())!=0:\n",
    "                word=pair2[0]\n",
    "                sentiment=pair2[1]\n",
    "                if word in modified_words:\n",
    "                    fout.write(\"#UNK\"+\" \"+sentiment+\"\\n\")\n",
    "                else:\n",
    "                    fout.write(word+\" \"+sentiment+\"\\n\")\n",
    "\n",
    "\n",
    "def emission_params(train_file):\n",
    "    with open(train_file, encoding = 'utf-8') as file:\n",
    "        emission_count= {}\n",
    "        label_count={}\n",
    "        for line in file:\n",
    "            pair = line.split()\n",
    "            if len(line.split())!=0:\n",
    "                #add 1 to count of (Xi, Yi)\n",
    "                word = pair[0]\n",
    "                sentiment = pair[1]\n",
    "                if word in emission_count.keys():\n",
    "                    if sentiment in emission_count[word].keys():\n",
    "                        emission_count[word][sentiment] +=1\n",
    "                    else:\n",
    "                        sentiments = emission_count[word]\n",
    "                        sentiments[sentiment] = 1\n",
    "                else:\n",
    "                    sentiment_count = {}\n",
    "                    sentiment_count[sentiment] = 1\n",
    "                    emission_count[word]=sentiment_count\n",
    "    \n",
    "                #add 1 to count of label Yi\n",
    "                if sentiment in label_count.keys():\n",
    "                    label_count[sentiment]+=1\n",
    "                else:\n",
    "                    label_count[sentiment]=1\n",
    "        for keya in emission_count.keys():\n",
    "            for keyb in emission_count[keya].keys():\n",
    "                emission_count[keya][keyb]/=(label_count[keyb]+1)\n",
    "        new_word = {}\n",
    "        for key in label_count.keys():\n",
    "            new_word[key] = 1/(label_count[key]+1)\n",
    "        emission_count['#UNK#'] = new_word\n",
    "       \n",
    "        return (emission_count,label_count)\n",
    "                          \n",
    "\n",
    "def sentiment_analysis(test_file,output_file,emission_params, label_count):\n",
    "    with open(test_file, encoding ='utf-8') as ifile, codecs.open(output_file, 'w', 'utf-8-sig') as ofile:\n",
    "        for line in ifile:\n",
    "            if len(line.split())!=0:\n",
    "                word = line.split()[0]\n",
    "                if word in emission_params.keys():\n",
    "                    value = emission_params[word]\n",
    "                    a = max(value,key=value.get)\n",
    "                    ofile.write(word+\" \"+a+'\\n')\n",
    "                else:\n",
    "                    value = emission_params['#UNK#']\n",
    "                    a = max(value,key=value.get)\n",
    "                    ofile.write(word+\" \"+a+'\\n')\n",
    "            else:\n",
    "                ofile.write('\\n')\n",
    "\n",
    "                \n",
    "modified_training_set(EN_train, EN_modified)                \n",
    "emission_params_EN, label_count_EN = emission_params(EN_modified)\n",
    "sentiment_analysis(EN_test,EN_output,emission_params_EN, label_count_EN)\n",
    "\n",
    "modified_training_set(FR_train, FR_modified)                \n",
    "emission_params_FR, label_count_FR = emission_params(FR_modified)\n",
    "sentiment_analysis(FR_test,FR_output,emission_params_FR, label_count_FR)\n",
    "\n",
    "modified_training_set(CN_train, CN_modified)                \n",
    "emission_params_CN,label_count_CN = emission_params(CN_modified)\n",
    "sentiment_analysis(CN_test,CN_output,emission_params_CN, label_count_CN)\n",
    "\n",
    "modified_training_set(SG_train, SG_modified)                \n",
    "emission_params_SG,label_count_SG = emission_params(SG_modified)\n",
    "sentiment_analysis(SG_test,SG_output,emission_params_SG, label_count_SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transition_params(train_file):\n",
    "    transition_count= {}\n",
    "    state_count={}\n",
    "    prev = 'START'\n",
    "    end = 'STOP'\n",
    "    state_count[prev] = 0\n",
    "    state_count[end] = 0\n",
    "    transition_count[end] = {}\n",
    "    with open(train_file, encoding = 'utf-8') as file:    \n",
    "        for line in file:\n",
    "            pair = line.split()\n",
    "            if len(pair)!= 0:\n",
    "                sentiment = pair[1]\n",
    "                # add prev to sentiment transition count\n",
    "                if sentiment in transition_count.keys():\n",
    "                    sentiment_list = transition_count[sentiment]\n",
    "                    if prev in sentiment_list.keys():\n",
    "                        sentiment_list[prev] += 1\n",
    "                    else:\n",
    "                        sentiment_list[prev] = 1\n",
    "                else:\n",
    "                    new_sentiment = {}\n",
    "                    new_sentiment[prev] = 1\n",
    "                    transition_count[sentiment] = new_sentiment\n",
    "\n",
    "                # add to start and stop state counts\n",
    "                if prev == 'START':\n",
    "                    state_count[prev] += 1\n",
    "                    state_count[end] += 1\n",
    "\n",
    "                # add to state count  \n",
    "                if sentiment in state_count.keys():\n",
    "                    state_count[sentiment]+=1\n",
    "                else:\n",
    "                    state_count[sentiment]=1\n",
    "              \n",
    "                prev = sentiment\n",
    "\n",
    "            else:\n",
    "                sentiment_list = transition_count[end]\n",
    "                if prev in sentiment_list.keys():\n",
    "                    sentiment_list[prev] +=1\n",
    "                else:\n",
    "                    sentiment_list[prev] =1   \n",
    "                prev = 'START'\n",
    "    for V in transition_count.keys():\n",
    "        for U in transition_count[V].keys():\n",
    "            transition_count[V][U] /= state_count[U]\n",
    "    return transition_count\n",
    "\n",
    "\n",
    "def viterbi_algo(test_file, output_file, transition_params, emission_params, labels):\n",
    "    sentences = []\n",
    "\n",
    "    with open(test_file, encoding ='utf-8') as ifile, codecs.open(output_file, 'w', 'utf-8-sig') as ofile:\n",
    "        sentence = []\n",
    "        for line in ifile:\n",
    "            if len(line.split())!=0:\n",
    "                sentence.append(line.split()[0])\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        \n",
    "        for s in sentences:\n",
    "            nodes = calculate_node_scores(s,transition_params, emission_params, labels)\n",
    "            labelled_sentence = backtracking(s,nodes)\n",
    "            for word in labelled_sentence:\n",
    "                ofile.write(word+'\\n')\n",
    "            ofile.write(\"\\n\")\n",
    "\n",
    "        \n",
    "def calculate_node_scores(s, transition_params, emission_params, labels):\n",
    "    nodes = {}\n",
    "    #base case\n",
    "    nodes[0] = {'START':[1,'nil']}\n",
    "    #recursive\n",
    "    for k in range (1, len(s)+1): #for each word\n",
    "        X = s[k-1]\n",
    "        for V in labels.keys(): #for each node\n",
    "            prev_nodes_dict = nodes[k-1] #access prev nodes\n",
    "            highest_score = 0\n",
    "            parent = 'nil'\n",
    "            #emission params\n",
    "            if X in emission_params.keys():\n",
    "                emission_labels = emission_params[X]\n",
    "\n",
    "                if V in emission_labels:\n",
    "                    b = emission_labels[V]\n",
    "                else:\n",
    "                    b = 0\n",
    "            else:\n",
    "                b = emission_params['#UNK#'][V]  \n",
    "                \n",
    "            for U in prev_nodes_dict.keys():\n",
    "                #transitionparams\n",
    "                prev_states = transition_params[V]\n",
    "                if U in prev_states.keys():\n",
    "                    a = prev_states[U]\n",
    "                else:\n",
    "                    a = 0\n",
    "                \n",
    "                #prev node score\n",
    "                prev_score = prev_nodes_dict[U][0]\n",
    "                score = prev_score*a*b\n",
    "                \n",
    "                if score>= highest_score:\n",
    "                    highest_score = score\n",
    "                    parent = U\n",
    "            if k in nodes.keys():\n",
    "                nodes[k][V] = [highest_score,parent]\n",
    "            else:\n",
    "                new_dict = {V:[highest_score,parent]}\n",
    "                nodes[k] = new_dict\n",
    "            \n",
    "    #end case\n",
    "    prev_nodes_dict = nodes[len(s)]\n",
    "    highest_score = 0\n",
    "    parent = 'nil'\n",
    "    for U in prev_nodes_dict.keys():\n",
    "        #transition\n",
    "        prev_states = transition_params['STOP']\n",
    "        if U in prev_states.keys():\n",
    "            a = prev_states[U]\n",
    "        else:\n",
    "            a = 0\n",
    "        #prev node score\n",
    "        prev_score = prev_nodes_dict[U][0]\n",
    "        score = prev_score*a\n",
    "        if score>= highest_score:\n",
    "            highest_score = score\n",
    "            parent = U\n",
    "    indiv_node = {'STOP': [highest_score,parent]}\n",
    "    nodes[len(s)+1]=indiv_node\n",
    "\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def backtracking(s, nodes):\n",
    "    prev_state = 'STOP'\n",
    "    for i in range(len(s)+1, 1,-1):\n",
    "        prev_node = nodes[i][prev_state]\n",
    "        prev_state = prev_node[1]\n",
    "        s[i-2] += \" \"+prev_state\n",
    "    return s\n",
    "\n",
    "transition_params_EN = transition_params(EN_train)\n",
    "viterbi_algo(EN_test, EN_viterbi, transition_params_EN, emission_params_EN, label_count_EN)\n",
    "transition_params_FR = transition_params(FR_train)\n",
    "viterbi_algo(FR_test, FR_viterbi, transition_params_FR, emission_params_FR, label_count_FR)\n",
    "transition_params_CN = transition_params(CN_train)\n",
    "viterbi_algo(CN_test, CN_viterbi, transition_params_CN, emission_params_CN, label_count_CN)\n",
    "transition_params_SG = transition_params(SG_train)\n",
    "viterbi_algo(SG_test, SG_viterbi, transition_params_SG, emission_params_SG, label_count_SG)\n",
    "\n",
    "# EN #\n",
    "# #Entity in gold data: 226\n",
    "# #Entity in prediction: 584\n",
    "# #Correct Entity: 109\n",
    "# Entity Precision: 0.1866\n",
    "# Entity recall: 0.4823\n",
    "# Entity F: 0.2691\n",
    "# #Correct Sentiment: 57\n",
    "# Sentiment precision: 0.0976\n",
    "# Sentiment recall: 0.2522\n",
    "# Sentiment F: 0.1407\n",
    "\n",
    "# FR #\n",
    "# #Entity in gold data: 223\n",
    "# #Entity in prediction: 582\n",
    "# #Correct Entity: 107\n",
    "# Entity Precision: 0.1838\n",
    "# Entity recall: 0.4798\n",
    "# Entity F: 0.2658\n",
    "# #Correct Sentiment: 60\n",
    "# Sentiment precision: 0.1031\n",
    "# Sentiment recall: 0.2691\n",
    "# Sentiment F: 0.1491\n",
    "\n",
    "# CN #\n",
    "# #Entity in gold data: 362\n",
    "# #Entity in prediction: 666\n",
    "# #Correct Entity: 92\n",
    "# Entity Precision: 0.1381\n",
    "# Entity recall: 0.2541\n",
    "# Entity F: 0.1790\n",
    "# #Correct Sentiment: 46\n",
    "# Sentiment precision: 0.0691\n",
    "# Sentiment recall: 0.1271\n",
    "# Sentiment F: 0.0895\n",
    "\n",
    "# SG #\n",
    "# #Entity in gold data: 1382\n",
    "# #Entity in prediction: 1559\n",
    "# #Correct Entity: 456\n",
    "# Entity Precision: 0.2925\n",
    "# Entity recall: 0.300\n",
    "# Entity F: 0.3101\n",
    "# #Correct Sentiment: 249\n",
    "# Sentiment precision: 0.1597\n",
    "# Sentiment recall: 0.1802\n",
    "# Sentiment F: 0.1693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
