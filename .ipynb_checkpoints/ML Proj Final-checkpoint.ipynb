{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "EN_train = \"./EN/train\"\n",
    "EN_test = \"./EN/dev.in\"\n",
    "EN_output = \"./EN/dev.p2.out\"\n",
    "EN_gold = \"./EN/dev.out\"\n",
    "EN_viterbi = \"./EN/dev.p3.out\"\n",
    "EN_modified = \"./EN/modified_train\"\n",
    "EN_maxmin = \"./EN/dev.p4.out\"\n",
    "\n",
    "CN_train = \"./CN/train\"\n",
    "CN_test = \"./CN/dev.in\"\n",
    "CN_output = \"./CN/dev.p2.out\"\n",
    "CN_gold = \"./CN/dev.out\"\n",
    "CN_modified = \"./CN/modified_train\"\n",
    "CN_viterbi = \"./CN/dev.p3.out\"\n",
    "\n",
    "SG_train = \"./SG/train\"\n",
    "SG_modified = \"./SG/modified_train\"\n",
    "SG_test = \"./SG/dev.in\"\n",
    "SG_output = \"./SG/dev.p2.out\"\n",
    "SG_gold = \"./SG/dev.out\"\n",
    "SG_viterbi = \"./SG/dev.p3.out\"\n",
    "\n",
    "FR_train = \"./FR/train\"\n",
    "FR_modified = \"./FR/modified_train\"\n",
    "FR_test = \"./FR/dev.in\"\n",
    "FR_output = \"./FR/dev.p2.out\"\n",
    "FR_gold = \"./FR/dev.out\"\n",
    "FR_viterbi = \"./FR/dev.p3.out\"\n",
    "FR_maxmin = \"./FR/dev.p4.out\"\n",
    "\n",
    "\n",
    "def modified_training_set(train_file,modified_train_set):\n",
    "    #fout=open ('modified_train_set','w')\n",
    "    with open (train_file, encoding='utf-8') as file:\n",
    "        tag_count={}\n",
    "        modified_words=[]\n",
    "        for line in file:\n",
    "            pair=line.split()\n",
    "            if len(line.split())!=0:\n",
    "                tag=pair[0]\n",
    "                observ=pair[1]\n",
    "                if tag in tag_count.keys():\n",
    "                    tag_count[tag]+=1\n",
    "                else:\n",
    "                    tag_count[tag]=1\n",
    "        for tag in tag_count:\n",
    "            if tag_count[tag]<3:\n",
    "                modified_words.append(tag)\n",
    "    with open (train_file, encoding='utf-8') as file2, codecs.open(modified_train_set, 'w', 'utf-8-sig') as fout:\n",
    "        for line in file2:\n",
    "            pair2=line.split()\n",
    "            if len(line.split())!=0:\n",
    "                word=pair2[0]\n",
    "                sentiment=pair2[1]\n",
    "                if word in modified_words:\n",
    "                    fout.write(\"#UNK\"+\" \"+sentiment+\"\\n\")\n",
    "                else:\n",
    "                    fout.write(word+\" \"+sentiment+\"\\n\")\n",
    "\n",
    "\n",
    "def emission_params(train_file):\n",
    "    with open(train_file, encoding = 'utf-8') as file:\n",
    "        emission_count= {}\n",
    "        label_count={}\n",
    "        for line in file:\n",
    "            pair = line.split()\n",
    "            if len(line.split())!=0:\n",
    "                #add 1 to count of (Xi, Yi)\n",
    "                word = pair[0]\n",
    "                sentiment = pair[1]\n",
    "                if word in emission_count.keys():\n",
    "                    if sentiment in emission_count[word].keys():\n",
    "                        emission_count[word][sentiment] +=1\n",
    "                    else:\n",
    "                        sentiments = emission_count[word]\n",
    "                        sentiments[sentiment] = 1\n",
    "                else:\n",
    "                    sentiment_count = {}\n",
    "                    sentiment_count[sentiment] = 1\n",
    "                    emission_count[word]=sentiment_count\n",
    "    \n",
    "                #add 1 to count of label Yi\n",
    "                if sentiment in label_count.keys():\n",
    "                    label_count[sentiment]+=1\n",
    "                else:\n",
    "                    label_count[sentiment]=1\n",
    "        for keya in emission_count.keys():\n",
    "            for keyb in emission_count[keya].keys():\n",
    "                emission_count[keya][keyb]/=(label_count[keyb]+1)\n",
    "        new_word = {}\n",
    "        for key in label_count.keys():\n",
    "            new_word[key] = 1/(label_count[key]+1)\n",
    "        emission_count['#UNK#'] = new_word\n",
    "       \n",
    "        return (emission_count,label_count)\n",
    "                          \n",
    "\n",
    "def sentiment_analysis(test_file,output_file,emission_params, label_count):\n",
    "    with open(test_file, encoding ='utf-8') as ifile, codecs.open(output_file, 'w', 'utf-8-sig') as ofile:\n",
    "        for line in ifile:\n",
    "            if len(line.split())!=0:\n",
    "                word = line.split()[0]\n",
    "                if word in emission_params.keys():\n",
    "                    value = emission_params[word]\n",
    "                    a = max(value,key=value.get)\n",
    "                    ofile.write(word+\" \"+a+'\\n')\n",
    "                else:\n",
    "                    value = emission_params['#UNK#']\n",
    "                    a = max(value,key=value.get)\n",
    "                    ofile.write(word+\" \"+a+'\\n')\n",
    "            else:\n",
    "                ofile.write('\\n')\n",
    "\n",
    "                \n",
    "modified_training_set(EN_train, EN_modified)                \n",
    "emission_params_EN, label_count_EN = emission_params(EN_modified)\n",
    "sentiment_analysis(EN_test,EN_output,emission_params_EN, label_count_EN)\n",
    "\n",
    "modified_training_set(FR_train, FR_modified)                \n",
    "emission_params_FR, label_count_FR = emission_params(FR_modified)\n",
    "sentiment_analysis(FR_test,FR_output,emission_params_FR, label_count_FR)\n",
    "\n",
    "modified_training_set(CN_train, CN_modified)                \n",
    "emission_params_CN,label_count_CN = emission_params(CN_modified)\n",
    "sentiment_analysis(CN_test,CN_output,emission_params_CN, label_count_CN)\n",
    "\n",
    "modified_training_set(SG_train, SG_modified)                \n",
    "emission_params_SG,label_count_SG = emission_params(SG_modified)\n",
    "sentiment_analysis(SG_test,SG_output,emission_params_SG, label_count_SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transition_params(train_file):\n",
    "    transition_count= {}\n",
    "    state_count={}\n",
    "    prev = 'START'\n",
    "    end = 'STOP'\n",
    "    state_count[prev] = 0\n",
    "    state_count[end] = 0\n",
    "    transition_count[end] = {}\n",
    "    with open(train_file, encoding = 'utf-8') as file:    \n",
    "        for line in file:\n",
    "            pair = line.split()\n",
    "            if len(pair)!= 0:\n",
    "                sentiment = pair[1]\n",
    "                # add prev to sentiment transition count\n",
    "                if sentiment in transition_count.keys():\n",
    "                    sentiment_list = transition_count[sentiment]\n",
    "                    if prev in sentiment_list.keys():\n",
    "                        sentiment_list[prev] += 1\n",
    "                    else:\n",
    "                        sentiment_list[prev] = 1\n",
    "                else:\n",
    "                    new_sentiment = {}\n",
    "                    new_sentiment[prev] = 1\n",
    "                    transition_count[sentiment] = new_sentiment\n",
    "\n",
    "                # add to start and stop state counts\n",
    "                if prev == 'START':\n",
    "                    state_count[prev] += 1\n",
    "                    state_count[end] += 1\n",
    "\n",
    "                # add to state count  \n",
    "                if sentiment in state_count.keys():\n",
    "                    state_count[sentiment]+=1\n",
    "                else:\n",
    "                    state_count[sentiment]=1\n",
    "              \n",
    "                prev = sentiment\n",
    "\n",
    "            else:\n",
    "                sentiment_list = transition_count[end]\n",
    "                if prev in sentiment_list.keys():\n",
    "                    sentiment_list[prev] +=1\n",
    "                else:\n",
    "                    sentiment_list[prev] =1   \n",
    "                prev = 'START'\n",
    "    for V in transition_count.keys():\n",
    "        for U in transition_count[V].keys():\n",
    "            transition_count[V][U] /= state_count[U]\n",
    "    return transition_count\n",
    "\n",
    "\n",
    "def viterbi_algo(test_file, output_file, transition_params, emission_params, labels):\n",
    "    sentences = []\n",
    "\n",
    "    with open(test_file, encoding ='utf-8') as ifile, codecs.open(output_file, 'w', 'utf-8-sig') as ofile:\n",
    "        sentence = []\n",
    "        for line in ifile:\n",
    "            if len(line.split())!=0:\n",
    "                sentence.append(line.split()[0])\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        \n",
    "        for s in sentences:\n",
    "            nodes = calculate_node_scores(s,transition_params, emission_params, labels)\n",
    "            labelled_sentence = backtracking(s,nodes)\n",
    "            for word in labelled_sentence:\n",
    "                ofile.write(word+'\\n')\n",
    "            ofile.write(\"\\n\")\n",
    "\n",
    "        \n",
    "def calculate_node_scores(s, transition_params, emission_params, labels):\n",
    "    nodes = {}\n",
    "    #base case\n",
    "    nodes[0] = {'START':[1,'nil']}\n",
    "    #recursive\n",
    "    for k in range (1, len(s)+1): #for each word\n",
    "        X = s[k-1]\n",
    "        for V in labels.keys(): #for each node\n",
    "            prev_nodes_dict = nodes[k-1] #access prev nodes\n",
    "            highest_score = 0\n",
    "            parent = 'nil'\n",
    "            #emission params\n",
    "            if X in emission_params.keys():\n",
    "                emission_labels = emission_params[X]\n",
    "\n",
    "                if V in emission_labels:\n",
    "                    b = emission_labels[V]\n",
    "                else:\n",
    "                    b = 0\n",
    "            else:\n",
    "                b = emission_params['#UNK#'][V]  \n",
    "                \n",
    "            for U in prev_nodes_dict.keys():\n",
    "                #transitionparams\n",
    "                prev_states = transition_params[V]\n",
    "                if U in prev_states.keys():\n",
    "                    a = prev_states[U]\n",
    "                else:\n",
    "                    a = 0\n",
    "                \n",
    "                #prev node score\n",
    "                prev_score = prev_nodes_dict[U][0]\n",
    "                score = prev_score*a*b\n",
    "                \n",
    "                if score>= highest_score:\n",
    "                    highest_score = score\n",
    "                    parent = U\n",
    "            if k in nodes.keys():\n",
    "                nodes[k][V] = [highest_score,parent]\n",
    "            else:\n",
    "                new_dict = {V:[highest_score,parent]}\n",
    "                nodes[k] = new_dict\n",
    "            \n",
    "    #end case\n",
    "    prev_nodes_dict = nodes[len(s)]\n",
    "    highest_score = 0\n",
    "    parent = 'nil'\n",
    "    for U in prev_nodes_dict.keys():\n",
    "        #transition\n",
    "        prev_states = transition_params['STOP']\n",
    "        if U in prev_states.keys():\n",
    "            a = prev_states[U]\n",
    "        else:\n",
    "            a = 0\n",
    "        #prev node score\n",
    "        prev_score = prev_nodes_dict[U][0]\n",
    "        score = prev_score*a\n",
    "        if score>= highest_score:\n",
    "            highest_score = score\n",
    "            parent = U\n",
    "    indiv_node = {'STOP': [highest_score,parent]}\n",
    "    nodes[len(s)+1]=indiv_node\n",
    "\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def backtracking(s, nodes):\n",
    "    prev_state = 'STOP'\n",
    "    for i in range(len(s)+1, 1,-1):\n",
    "        prev_node = nodes[i][prev_state]\n",
    "        prev_state = prev_node[1]\n",
    "        s[i-2] += \" \"+prev_state\n",
    "    return s\n",
    "\n",
    "transition_params_EN = transition_params(EN_train)\n",
    "viterbi_algo(EN_test, EN_viterbi, transition_params_EN, emission_params_EN, label_count_EN)\n",
    "transition_params_FR = transition_params(FR_train)\n",
    "viterbi_algo(FR_test, FR_viterbi, transition_params_FR, emission_params_FR, label_count_FR)\n",
    "transition_params_CN = transition_params(CN_train)\n",
    "viterbi_algo(CN_test, CN_viterbi, transition_params_CN, emission_params_CN, label_count_CN)\n",
    "transition_params_SG = transition_params(SG_train)\n",
    "viterbi_algo(SG_test, SG_viterbi, transition_params_SG, emission_params_SG, label_count_SG)\n",
    "\n",
    "# EN #\n",
    "# #Entity in gold data: 226\n",
    "# #Entity in prediction: 584\n",
    "# #Correct Entity: 109\n",
    "# Entity Precision: 0.1866\n",
    "# Entity recall: 0.4823\n",
    "# Entity F: 0.2691\n",
    "# #Correct Sentiment: 57\n",
    "# Sentiment precision: 0.0976\n",
    "# Sentiment recall: 0.2522\n",
    "# Sentiment F: 0.1407\n",
    "\n",
    "# FR #\n",
    "# #Entity in gold data: 223\n",
    "# #Entity in prediction: 582\n",
    "# #Correct Entity: 107\n",
    "# Entity Precision: 0.1838\n",
    "# Entity recall: 0.4798\n",
    "# Entity F: 0.2658\n",
    "# #Correct Sentiment: 60\n",
    "# Sentiment precision: 0.1031\n",
    "# Sentiment recall: 0.2691\n",
    "# Sentiment F: 0.1491\n",
    "\n",
    "# CN #\n",
    "# #Entity in gold data: 362\n",
    "# #Entity in prediction: 666\n",
    "# #Correct Entity: 92\n",
    "# Entity Precision: 0.1381\n",
    "# Entity recall: 0.2541\n",
    "# Entity F: 0.1790\n",
    "# #Correct Sentiment: 46\n",
    "# Sentiment precision: 0.0691\n",
    "# Sentiment recall: 0.1271\n",
    "# Sentiment F: 0.0895\n",
    "\n",
    "# SG #\n",
    "# #Entity in gold data: 1382\n",
    "# #Entity in prediction: 1559\n",
    "# #Correct Entity: 456\n",
    "# Entity Precision: 0.2925\n",
    "# Entity recall: 0.300\n",
    "# Entity F: 0.3101\n",
    "# #Correct Sentiment: 249\n",
    "# Sentiment precision: 0.1597\n",
    "# Sentiment recall: 0.1802\n",
    "# Sentiment F: 0.1693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Part 4\n",
    "def maxmin_topk(test_file, output_file, transition_params, emission_params, labels, top_k, i_th):\n",
    "    sentences = []\n",
    "\n",
    "    with open(test_file, encoding ='utf-8') as ifile, codecs.open(output_file, 'w', 'utf-8-sig') as ofile:\n",
    "        sentence = []\n",
    "        for line in ifile:\n",
    "            if len(line.split())!=0:\n",
    "                sentence.append(line.split()[0])\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        \n",
    "        for s in sentences:\n",
    "            nodes = calculate_topk_node_scores(s,transition_params, emission_params, labels, top_k)\n",
    "            labelled_sentence = backtracking_topk(s,nodes, i_th)\n",
    "            for word in labelled_sentence:\n",
    "                ofile.write(word+'\\n')\n",
    "            ofile.write(\"\\n\")\n",
    "\n",
    "\n",
    "def calculate_topk_node_scores(s, transition_params, emission_params, labels, top_k):\n",
    "    nodes = {}\n",
    "    #base case\n",
    "    nodes[0] = {'START':[[1,'nil',0]]}\n",
    "    #recursive\n",
    "    for k in range (1, len(s)+1): #for each word\n",
    "        X = s[k-1]\n",
    "        for V in labels.keys(): #for each node\n",
    "            prev_nodes_dict = nodes[k-1] #access prev nodes\n",
    "            #emission params\n",
    "            if X in emission_params.keys():\n",
    "                emission_labels = emission_params[X]\n",
    "\n",
    "                if V in emission_labels:\n",
    "                    b = emission_labels[V]\n",
    "                else:\n",
    "                    b = 0\n",
    "            else:\n",
    "                b = emission_params['#UNK#'][V]  \n",
    "            scores = []\n",
    "            for U in prev_nodes_dict.keys():\n",
    "                #transitionparams\n",
    "                prev_states = transition_params[V]\n",
    "                if U in prev_states.keys():\n",
    "                    a = prev_states[U]\n",
    "                else:\n",
    "                    a = 0\n",
    "                index = 0\n",
    "                for prev_k_nodes in prev_nodes_dict[U]:\n",
    "                    #prev node score\n",
    "                    score = prev_k_nodes[0]*a*b\n",
    "                    scores.append([score, U, index])\n",
    "                    index += 1\n",
    "            \n",
    "            #take top k scores\n",
    "            scores.sort(key=lambda x: x[0],reverse=True)\n",
    "            topk_scores = scores[:top_k]\n",
    "            if k in nodes.keys():\n",
    "                nodes[k][V] = topk_scores\n",
    "            else:\n",
    "                new_dict = {V:topk_scores}\n",
    "                nodes[k] = new_dict\n",
    "            \n",
    "    #end case\n",
    "    prev_nodes_dict = nodes[len(s)]\n",
    "    scores = []\n",
    "    for U in prev_nodes_dict.keys():\n",
    "        #transition\n",
    "        prev_states = transition_params['STOP']\n",
    "        if U in prev_states.keys():\n",
    "            a = prev_states[U]\n",
    "        else:\n",
    "            a = 0\n",
    "        #prev node score\n",
    "        index = 0\n",
    "        for prev_k_nodes in prev_nodes_dict[U]:\n",
    "            score = prev_k_nodes[0]*a\n",
    "            scores.append([score, U, index])\n",
    "            index += 1\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    topk_scores = scores[:top_k]\n",
    "    indiv_node = {'STOP': topk_scores}\n",
    "    nodes[len(s)+1]=indiv_node\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "\n",
    "def backtracking_topk(s, nodes, i_th):\n",
    "    prev_state = 'STOP'\n",
    "    prev_index = 0\n",
    "    for i in range(len(s)+1, 1,-1):\n",
    "        if i==len(s)+1:\n",
    "            prev_node = nodes[i][prev_state][i_th-1]\n",
    "        else:\n",
    "            prev_node = nodes[i][prev_state][prev_index]\n",
    "        prev_state = prev_node[1]\n",
    "        prev_index = prev_node[2]\n",
    "        s[i-2] += \" \"+prev_state\n",
    "    return s\n",
    "\n",
    "maxmin_topk(EN_test, EN_maxmin, transition_params_EN, emission_params_EN, label_count_EN, 7, 1)\n",
    "maxmin_topk(FR_test, FR_maxmin, transition_params_FR, emission_params_FR, label_count_FR, 7, 1)\n",
    "\n",
    "\n",
    "def fwd_bkw(s, labels, transition_params, emission_params):\n",
    "    # forward part of the algorithm\n",
    "    end_st='STOP'\n",
    "    fwd = []\n",
    "    f_prev = {}\n",
    "    for i, observation_i in enumerate(s):\n",
    "        f_curr = {}\n",
    "        for st in labels:\n",
    "            if i == 0:\n",
    "                # base case for the forward part #TODO\n",
    "                prev_f_sum = transition_params[0][st]\n",
    "            else:\n",
    "                prev_f_sum = sum(f_prev[k]*transition_params[k][st] for k in labels)\n",
    "\n",
    "            f_curr[st] = emission_params[st][observation_i] * prev_f_sum\n",
    "\n",
    "        fwd.append(f_curr)\n",
    "        f_prev = f_curr\n",
    "\n",
    "    p_fwd = sum(f_curr[k] * transition_params[k][end_st] for k in labels)\n",
    "\n",
    "    # backward part of the algorithm\n",
    "    bkw = []\n",
    "    b_prev = {}\n",
    "    for i, observation_i_plus in enumerate(reversed(s[1:]+(None,))):\n",
    "        b_curr = {}\n",
    "        for st in labels:\n",
    "            if i == 0:\n",
    "                # base case for backward part\n",
    "                b_curr[st] = transition_params[st][end_st]\n",
    "            else:\n",
    "                b_curr[st] = sum(transition_params[st][l] * emission_params[l][observation_i_plus] * b_prev[l] for l in labels)\n",
    "\n",
    "        bkw.insert(0,b_curr)\n",
    "        b_prev = b_curr\n",
    "#Start prob here too\n",
    "    p_bkw = sum(transition_params[0][st]* emission_params[l][s[0]] * b_curr[l] for l in labels)\n",
    "\n",
    "    # merging the two parts\n",
    "    posterior = []\n",
    "    for i in range(len(s)):\n",
    "        posterior.append({st: fwd[i][st] * bkw[i][st] / p_fwd for st in labels})\n",
    "\n",
    "    assert p_fwd == p_bkw\n",
    "    return fwd, bkw, posterior\n",
    "\n",
    "def maxmin_topk2(test_file, output_file, transition_params, emission_params, labels):\n",
    "    sentences = []\n",
    "\n",
    "    with open(test_file, encoding ='utf-8') as ifile, codecs.open(output_file, 'w', 'utf-8-sig') as ofile:\n",
    "        sentence = []\n",
    "        for line in ifile:\n",
    "            if len(line.split())!=0:\n",
    "                sentence.append(line.split()[0])\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        \n",
    "        for s in sentences:\n",
    "            nodes = fwd_bkw(s,labels,transition_params, emission_params)\n",
    "maxmin_topk2(EN_test, EN_maxmin2, transition_params_EN, emission_params_EN, label_count_EN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 of 1:\n",
      "score for 101\n",
      "b'CN\\nEN\\nFR\\nInstruction.txt\\nML Proj Final.ipynb\\nML Project Part 3.ipynb\\nML Project Q2_FinalFinal.ipynb\\nProject.pdf\\nREADME.md\\nSG\\ndev.out\\ndev.prediction\\nevalResult.py\\n'\n"
     ]
    }
   ],
   "source": [
    "#part 5\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import copy\n",
    "\n",
    "def remove_tag(training_file): # generate train_no_tag file\n",
    "    f = open(training_file,\"r\",encoding=\"utf8\")\n",
    "    new_file = training_file+\"_perceptron_100\"\n",
    "    f1 = open(new_file,\"w+\",encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        if len(line)>1:\n",
    "            f1.write(line[0]+\"\\n\")\n",
    "        else:\n",
    "            f1.write(\"\\n\")\n",
    "\n",
    "    f.close()\n",
    "    f1.close()\n",
    "    return\n",
    "\n",
    "def para(training_file): # output a dictionary of emission_parameter\n",
    "    cnt_y = Counter() # count the y tags\n",
    "    e_para = Counter() # count the y|x tags\n",
    "    t_para = Counter()\n",
    "    state = \"start\"\n",
    "    y0_y1 = \"\"\n",
    "    # init_e_para = Counter()\n",
    "    # init_t_para = Counter()\n",
    "    f = open(training_file, \"r\", encoding='utf8')\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        if (len(line) == 0):\n",
    "            y0_y1 = state + \" stop\"\n",
    "            state = \"stop\"\n",
    "        else:\n",
    "            e_para[line]+=1\n",
    "            line = line.split()[1]\n",
    "            y0_y1 = state + \" \" + line\n",
    "            state = line\n",
    "            cnt_y[line]+=1\n",
    "        t_para[y0_y1]+=1\n",
    "\n",
    "    for tag in cnt_y:\n",
    "        e_para[\"new_word \"+tag]=1\n",
    "\n",
    "    f.close()\n",
    "    return  e_para, t_para\n",
    "\n",
    "def init_para(training_file):\n",
    "    cnt_y = Counter() # count the y tags\n",
    "    e_para = Counter() # count the y|x tags\n",
    "    t_para = Counter()\n",
    "    state = \"start\"\n",
    "    y0_y1 = \"\"\n",
    "    # init_e_para = Counter()\n",
    "    # init_t_para = Counter()\n",
    "    f = open(training_file, \"r\", encoding='utf8')\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        if (line == \"\"):\n",
    "            y0_y1 = state + \" stop\"\n",
    "            state = \"stop\"\n",
    "        else:\n",
    "            e_para[line]=0\n",
    "            line = line.split()[1]\n",
    "            y0_y1 = state + \" \" + line\n",
    "            state = line\n",
    "            cnt_y[line]=0\n",
    "        t_para[y0_y1]=0\n",
    "\n",
    "    for tag in cnt_y:\n",
    "        e_para[\"new_word \"+tag]=0\n",
    "\n",
    "    f.close()\n",
    "    return  e_para, t_para\n",
    "\n",
    "def test_phase_perceptron(test_file,emission_parameter,transition_parameter,top_k=1, run = 1):\n",
    "    tag = [\"O\",\"B-neutral\",\"B-positive\",\"B-negative\",\"I-neutral\",\"I-positive\",\"I-negative\"]\n",
    "    step_k = 1\n",
    "    v_label = \"O\"\n",
    "    u_label = \"start\"\n",
    "    optimal_value = -1\n",
    "    pi = [step_k,v_label,optimal_value,u_label]\n",
    "    sentence = []\n",
    "    output = []\n",
    "    list_u = [0,1,2,3,4,5,6]\n",
    "    dict_u = {}\n",
    "    state = (step_k, list_u)\n",
    "    data_x = set()\n",
    "    for entry in emission_parameter.keys():\n",
    "        entry = entry.split(' ')[0]\n",
    "        data_x.add(entry)\n",
    "\n",
    "\n",
    "    file_dir = test_file[:-3]+str(int(test_file[-3:])+run)\n",
    "    f = open(test_file, \"r\", encoding='utf8')\n",
    "    f1 = open(file_dir, \"w+\", encoding='utf8')\n",
    "\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        if line!=\"\":\n",
    "            for id_v,v in enumerate(tag):\n",
    "                best = -1\n",
    "                best_add = -1\n",
    "                for id_u,u in enumerate(state[1]): #enumerate list_u stored in state\n",
    "                    if(line in data_x):\n",
    "                        b_word = line + \" \" + v\n",
    "                        b_value = emission_parameter[b_word]\n",
    "                    else:\n",
    "                        b_word = \"new_word \" + v\n",
    "                        b_value = emission_parameter[b_word]\n",
    "\n",
    "                    if state[0] == 1:\n",
    "                        u_label = \"start\"\n",
    "                        uv_label = u_label+\" \" + v\n",
    "                        a_uv_value = transition_parameter[uv_label]\n",
    "                        optimal_value = a_uv_value+b_value\n",
    "\n",
    "                    else:\n",
    "                        u_label = u[1]\n",
    "                        uv_label = u_label + \" \" + v\n",
    "                        a_uv_value = transition_parameter[uv_label]\n",
    "                        optimal_value =  u[2] + a_uv_value + b_value\n",
    "\n",
    "                    if optimal_value>best:\n",
    "                        best = optimal_value\n",
    "                        pi = [step_k, v, best, u_label]\n",
    "\n",
    "                dict_u[(step_k,v)] = pi[3]\n",
    "                list_u[id_v] = pi\n",
    "\n",
    "            sentence.insert(0,line)\n",
    "            list_u2 = copy.deepcopy(list_u)\n",
    "            step_k+=1\n",
    "            state = (step_k,list_u2)\n",
    "\n",
    "        else: # when it reaches the empty line\n",
    "            best = -1\n",
    "            for id_u,u in enumerate(state[1]):\n",
    "                uv_label = u[1] + \" stop\"\n",
    "                a_uv_value = transition_parameter[uv_label]\n",
    "                optimal_value = u[2]+a_uv_value\n",
    "                pi = [state[0],\"stop\",optimal_value,u[1]]\n",
    "                list_u[id_u] = pi\n",
    "\n",
    "\n",
    "            list_u = sorted(list_u,key=itemgetter(2),reverse=True)  # arrange by descending order to choose top_k\n",
    "            u_label = list_u[top_k-1][3]\n",
    "            output.append(sentence[0]+\" \"+u_label+\"\\n\")\n",
    "\n",
    "            for i in range(1,len(sentence)): # back tracking to find the y_labels\n",
    "                search_key = (step_k-i,u_label)\n",
    "                u_label = dict_u[search_key]\n",
    "                output.insert(0,sentence[i]+\" \"+u_label+\"\\n\")\n",
    "\n",
    "            for i in output:\n",
    "                f1.write(i)\n",
    "            f1.write(\"\\n\")\n",
    "            sentence = []\n",
    "            output=[]\n",
    "            dict_u={}\n",
    "            step_k=1\n",
    "            state = (step_k,[0,1,2,3,4,5,6])\n",
    "    f1.close()\n",
    "    return\n",
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def update_para1(training_file, n = 3):\n",
    "    parameter1 = para(training_file)\n",
    "    e_c1 = copy.deepcopy(parameter1[0])\n",
    "    t_c1 = copy.deepcopy(parameter1[1])\n",
    "    init_parameter = init_para(training_file) #initialize epara and tpara to 0\n",
    "    e_para = copy.deepcopy(init_parameter[0])\n",
    "    t_para = copy.deepcopy(init_parameter[1])\n",
    "\n",
    "    remove_tag(training_file) # generate training file without tags\n",
    "    blank_file = training_file+\"_perceptron_100\"\n",
    "    for run_num in range(n):\n",
    "        print(\"Running %d of %d:\" %(run_num+1,n) )\n",
    "        test_phase_perceptron(blank_file,e_para,t_para,run = run_num+1) # generates prediction file\n",
    "        predicted_file = blank_file[:-3]+str(int(blank_file[-3:])+run_num+1)\n",
    "        parameter2 = para(predicted_file) # to get e_c2, t_c2\n",
    "        e_c2 = copy.deepcopy(parameter2[0])\n",
    "        t_c2 = copy.deepcopy(parameter2[1])\n",
    "        \n",
    "        # follow formula - e_para = e_para+c1-c2\n",
    "        e_para += e_c1  \n",
    "        e_para.subtract(e_c2)\n",
    "\n",
    "        t_para += t_c1\n",
    "        t_para.subtract(t_c2)\n",
    "        proc = subprocess.Popen([\"ls\"], stdout=subprocess.PIPE, shell=True)\n",
    "        (out,err) = proc.communicate()\n",
    "        print (\"score for \"+str(int(blank_file[-3:])+run_num+1))\n",
    "        print (out)\n",
    "        #\"python\", \"evalResult.py\",\"train\", \"train_perceptron_\"+str(int(blank_file[-3:])+run_num+1)\n",
    "        \n",
    "\n",
    "    return e_para ,t_para\n",
    "\n",
    "def update_para2(training_file,e_para,t_para, n = 3):\n",
    "    parameter1 = para(training_file)\n",
    "    e_c1 = copy.deepcopy(parameter1[0])\n",
    "    t_c1 = copy.deepcopy(parameter1[1])\n",
    "\n",
    "    remove_tag(training_file) # generate training file without tags\n",
    "    blank_file = training_file+\"_perceptron_100\"\n",
    "    for run_num in range(n):\n",
    "        print(\"Running %d of %d:\" %(run_num+1,n) )\n",
    "        test_phase_perceptron(blank_file,e_para,t_para,run = run_num+1)\n",
    "        predicted_file = blank_file[:-3]+str(int(blank_file[-3:])+run_num+1)\n",
    "        parameter2 = para(predicted_file)\n",
    "        e_c2 = copy.deepcopy(parameter2[0])\n",
    "        t_c2 = copy.deepcopy(parameter2[1])\n",
    "\n",
    "        e_para += e_c1\n",
    "        e_para.subtract(e_c2)\n",
    "\n",
    "        t_para += t_c1\n",
    "        t_para.subtract(t_c2)\n",
    "\n",
    "    return e_para ,t_para\n",
    "\n",
    "\n",
    "import os\n",
    "training_file = \"./EN/train\"\n",
    "# test_file = \"./dev.in\"\n",
    "parameter = update_para1(training_file, n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
